# -*- coding: utf-8 -*-
"""Contract Clause Analyser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jBHPyI69VKJg4D5Yly13wY1wSIdHRyaa
"""



import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
import seaborn as sns
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import RandomOverSampler
import re
from imblearn.over_sampling import ADASYN
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import ADASYN
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings("ignore")

contract_data = pd.read_csv("legal_contract_clauses.csv")

print(contract_data.head())

print(contract_data.info())

print(contract_data['risk_level'].value_counts())

contract_data.isna().sum()

X = contract_data['clause_text']
y = contract_data['risk_level']

# Encode risk levels into numbers
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split dataset (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print(f"Training samples: {len(X_train)}, Testing samples: {len(X_test)}")

def clean_text(text):
  test = text.lower()
  text = re.sub(r'[^a-zA-Z0-9\s]','',text)
  text = re.sub(r'\s+',' ',text).strip()
  return text

X_train_clean = X_train.apply(clean_text)
X_test_clean = X_test.apply(clean_text)

tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train_tfidf = tfidf.fit_transform(X_train_clean)
X_test_tfidf = tfidf.transform(X_test_clean)

ros = RandomOverSampler(random_state=42)
X_res, y_res = ros.fit_resample(X_train_tfidf, y_train)

from collections import Counter
print("Balanced class distribution:", Counter(y_res))

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# Initialize individual models
clf1 = LogisticRegression(max_iter=1000, random_state=42)
clf2 = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Combine into a Voting Classifier (soft voting recommended for probabilities)
voting_clf = VotingClassifier(
    estimators=[('lr', clf1), ('xgb', clf2)],
    voting='soft',  # uses predicted probabilities
    n_jobs=-1
)

# Train on balanced training data
voting_clf.fit(X_res, y_res)

# Predict on test set
y_pred = voting_clf.predict(X_test_tfidf)

from sklearn.metrics import classification_report

# Convert numeric labels back to original strings
y_test_labels = le.inverse_transform(y_test)
y_pred_labels = le.inverse_transform(y_pred)

# Detailed performance metrics
print(classification_report(y_test_labels, y_pred_labels))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_, cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test_labels, y_pred_labels)
print(f"Test Accuracy: {accuracy:.2f}")

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

"""##Stacking

"""

# Base learners
base_learners = [
    ('lr', LogisticRegression(max_iter=1000, random_state=42)),
    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))
]

# Meta-learner
meta_learner = RandomForestClassifier(n_estimators=100, random_state=42)
stack_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5,    # 5-fold cross-validation for meta-model training
    n_jobs=-1
)

stack_clf.fit(X_res, y_res)

y_pred_stack = stack_clf.predict(X_test_tfidf)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Accuracy
print(f"Stacking Test Accuracy: {accuracy_score(y_test, y_pred_stack):.2f}")

# Classification report
print(classification_report(y_test, y_pred_stack, target_names=le.classes_))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_stack)
sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_, cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import accuracy_score

# Overall accuracy
overall_accuracy = accuracy_score(y_test, y_pred_stack)
print(f"Stacking Model Overall Accuracy: {overall_accuracy:.2f}")

# Save the stacking classifier
joblib.dump(stack_clf, 'legal_risk_stacking_model.pkl')

# Save the TF-IDF vectorizer
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')

# Save the label encoder
joblib.dump(le, 'label_encoder.pkl')

"""## Model done till here

"""

